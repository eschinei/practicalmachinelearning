---
title: "ML Course Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## ML Capstone
### Objective
The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. 

### Load Data
The first step is to load the data provided and do some exploratory analysis to understand the data we are dealing with:
```{r load, warning=FALSE, message=FALSE }
library(caret)
library(randomForest)
library(rpart)

validation = read.csv("C:/Users/Martuki/Desktop/pml-testing.csv")
training = read.csv("C:/Users/Martuki/Desktop/pml-training.csv")

dim(training)
str(training)
```

Just for curiosity I inspect the frequency of each type of classe and we notice they are on the same range (beign A the most frequent):
```{r freq}
barplot(prop.table(table(training$classe)))
```

### Clean data
We can notice there are many variables we do not need hence we can remove the first fields which are not relevant for our study, remove all empty fields and finally remove those variable with variance close to 0 (using nearZeroVar):
```{r clean}
# remove non relevant columns
sub_training = training [,-c(1:7)]

# remove columns with NA
sub_training = sub_training [,colSums(is.na(sub_training)) == 0]

# remove variables with Var near 0
nzv = nearZeroVar(sub_training, saveMetrics = T)
sub_training = sub_training[,rownames(nzv[nzv$nzv=="FALSE",])]
```

Now we are ready to subset our data for cross validation:
```{r crossval}
# Data Partition
inTrain = createDataPartition (y=sub_training$classe, p=0.7, list=F)
training = sub_training[inTrain,]
testing = sub_training [-inTrain,]
```

# Build Models
I have tested three different models methods in order to find the most accurate for our data: Random Forest, Regression Trees and Linear Discriminant Analysis. I have built their models, predict using our subset of testing and compared their accuracy:
```{r models}
# Random Forest
set.seed(125)
modRF = randomForest(classe ~. , data=training, method="class")
predRF = predict(modRF,newdata=testing)
confusionMatrix(predRF, testing$classe)

# Tree
modRpart = train(classe~.,method="rpart", data=training)
predRpart = predict(modRpart,newdata=testing)
confusionMatrix(predRpart, testing$classe)

# LDA
modLDA = train(classe~.,method="lda", data=training)
predLDA = predict(modLDA,newdata=testing)
confusionMatrix(predLDA, testing$classe)
```

# Cross Validation
As we can see RF provides the highest accuracy compared to the other two models with 99.49%, hence the expected out-of-sample error is about 0.0051, or 0.51%, so RF is the best model for our data and I use it for cross validation:

```{r pred}
# Prediction validation
predRFV = predict(modRF, newdata=validation)
predRFV
```
